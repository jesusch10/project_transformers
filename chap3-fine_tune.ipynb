{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4cb9d19-2a83-4baf-8cce-dc2c3a7d8931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 12:04:58.633128: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748513098.690452    2158 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748513098.708221    2158 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748513098.843567    2158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748513098.843605    2158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748513098.843607    2158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748513098.843608    2158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-29 12:04:58.869527: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b261e-ece0-4486-b556-b3bc0925e757",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e974d5-ed2d-4bd7-9fc0-d8222af3aa72",
   "metadata": {},
   "source": [
    "### Handling multiple sequences - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56909ce3-d2d6-47e1-8e39-cf4b4863e759",
   "metadata": {},
   "source": [
    "Datasets: https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "016148cd-c470-4130-995c-4133b88ea6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading dataset\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9bbce9-4e40-42ad-9ee5-37cd7224322e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing dataset\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]\n",
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2ccd7-bc92-48a1-a2b3-d80b9465610f",
   "metadata": {},
   "source": [
    "Labels are already integers, so not preprocessing needed.  \n",
    "Behind the scenes, label is of type ClassLabel, and the mapping of integers to label name is stored in the names folder. 0 corresponds to not_equivalent, and 1 corresponds to equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d09b48-31ea-460f-9336-fc760349be60",
   "metadata": {},
   "source": [
    "From previous notebook, however, tokenizer returns a dictionary (a lot of RAM), so a new function is defined to allow ```batched=True``` in ```map()``` function to speed up tokenization since:\n",
    "- Applies multiprocessing to go faster than applying the function on each element of the dataset.\n",
    "- Saves results in cache as soon as one element is processed, so memory is not overloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47ead39-21dd-44c1-8160-d01e48905e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79991c9faddc4cb3b95979c83d8b65af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Speeding up tokenization\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence1\"], example[\"sentence2\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    ")\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=drug_dataset[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd013d45-9828-4efc-a3ea-6500553cf4b7",
   "metadata": {},
   "source": [
    "```padding``` is skipped because it is not efficient: it’s better to pad the samples when we’re building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset.  \n",
    "```num_proc``` argument allows multiprocessing; not used here because Tokenizers library already uses multiple threads to tokenize our samples faster.\n",
    "```return_overflowing_tokens=True``` is useful for long documents to not drop the truncated remainder of the sequence, but rather split it into multiple input chunks for the model. It also returns additional information about the overflow.\n",
    "```remove_columns``` removes the columns from the old dataset, which is necessary because the Tokenizer extends the number of columns (due to the chunks) regarding the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ffddd0-2544-4ad7-bd3c-586de462a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mismatched length problem can be also solved by making the old columns the same size as the new ones\n",
    "def tokenize_function2(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    # Extract mapping between new and old indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function2, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f4cd00c-0443-44f9-9b98-2d79dcd17539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different lengths [50, 59, 47, 67, 59, 50, 62, 32]\n",
      "After padding {'input_ids': torch.Size([8, 67]), 'token_type_ids': torch.Size([8, 67]), 'attention_mask': torch.Size([8, 67]), 'labels': torch.Size([8])}\n"
     ]
    }
   ],
   "source": [
    "# Dynamic padding on each batch with collate function (not for TPU, which prefers all batches with same size)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "samples = tokenized_datasets[\"train\"][:8] # let’s grab a few samples\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "print('Different lengths', [len(x) for x in samples[\"input_ids\"]])\n",
    "batch = data_collator(samples)\n",
    "print('After padding', {k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f02bbf-0687-41e4-8817-e7782f8aed22",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9b655b-121e-4f5c-910c-0f749cafd4c8",
   "metadata": {},
   "source": [
    "### Fine-tuning with Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "133b340a-2e9a-41d8-b3a1-b75595e015fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesusch10/miniconda3/envs/project_transformers_env/lib/python3.10/site-packages/transformers/training_args.py:1577: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Getting configuration object that defines how your model will be trained, and the pretrained model\n",
    "training_args = TrainingArguments(\"test-trainer\", no_cuda=True) # Using CPU instead of GPU, which is not recommended, but I lack resources\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4922460-7f7b-4f5d-a101-60e7322d68e9",
   "metadata": {},
   "source": [
    "About the warning: BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "657aa089-f241-416a-bc85-407b9c8b8aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.5873920520146688, metrics={'train_runtime': 30.1609, 'train_samples_per_second': 0.796, 'train_steps_per_second': 0.099, 'total_flos': 826333158240.0, 'train_loss': 0.5873920520146688, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining evaluation function, trainer, and fine-tuning\n",
    "\n",
    "def compute_metrics(eval_preds): # Input is a tuple with a predictions field and a label_ids field\n",
    "    \"\"\"Look text below for explanation\"\"\"\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\") # Loads evaluation metric used for the GLUE benchmark, specifically for the MRPC task\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1) # Gives the predicted class for each row (sample\n",
    "    return metric.compute(predictions=predictions, references=labels) # Dict mapping strings (metrics) to floats (values)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].select(range(8)), # Using a fraction of the dataset, so training is fast\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].select(range(8)),\n",
    "    # data_collator=data_collator, # when you pass a tokenizer as the processing_class, the default data_collator used by the Trainer will be a DataCollatorWithPadding, so this line is skipped\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics, # Optional, but necessary to report metrics at the end of each epoch\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135bd5a-44aa-4db8-a310-62c8efddf1a9",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e4a46-c63c-4015-aa6d-16b37459521c",
   "metadata": {},
   "source": [
    "### Fine-tuning without Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c44203d4-1f95-4b38-b8b0-cca213f2a639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Postprocessing tokenized_datasets to take care of some things that the Trainer did automatically\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1e1f2a3-503b-4652-9b2e-01d444fab54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([4]),\n",
       " 'input_ids': torch.Size([4, 67]),\n",
       " 'token_type_ids': torch.Size([4, 67]),\n",
       " 'attention_mask': torch.Size([4, 67])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining dataloaders and inspecting them\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"].select(range(8)), shuffle=True, batch_size=4, collate_fn=data_collator # batch_size <= dataset size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"].select(range(8)), batch_size=4, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc76520-81d3-460b-9cb9-2483f8ad0fc4",
   "metadata": {},
   "source": [
    "Shapes will probably be slightly different since ```shuffle=True``` and padding to the maximum length inside the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c89c64ac-c3eb-4b20-8bb4-64d95e33a5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4203, grad_fn=<NllLossBackward0>) torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "# Checking if everything is alright\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f86acb-33e1-4830-8d58-837dd4f10c27",
   "metadata": {},
   "source": [
    "Yes! Loss and logits (two for each input in our batch, so a tensor of size 8 x 2) obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c4430db-9c01-49ef-8e4b-adcecad235af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an optimizer and a learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler( # learning rate scheduler used is a linear decay from the maximum value (5e-5) to 0\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82d7fb23-f3f0-45c7-84fa-89baa11363a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting CPU or GPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d706b1bd-22af-4406-931c-75465fa51045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a51317a7c94f9294c7f1affcf831f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training finally!\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train() # Sets the model to training mode\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()} # Moves the batch (a dict of inputs, attention masks, labels, etc.) to the GPU or CPU.\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward() # Computes gradients (how much each parameter contributed to the loss)\n",
    "        optimizer.step() # Updates the model’s weights using the gradients: model parameters are tensors stored in the model, to which the Optimizer holds references\n",
    "        lr_scheduler.step() # Adjusts the learning rate based on the current step (optional but improves training)\n",
    "        optimizer.zero_grad() # Clears the gradients from the previous step (essential to avoid accumulation)\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a397ed1-da71-4a46-85e6-78b3403ddc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.375, 'f1': 0.5454545454545454}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad(): # No gradients are computed, which speeds up evaluation\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5aeada-5a7d-4c5a-bc1c-d6223332483a",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093fe09-d5f4-4156-9df0-e38b79a9fa43",
   "metadata": {},
   "source": [
    "### Fine-tuning with Accelerate library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f753c9-9b7f-45a4-9ae4-1c413db5de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enabling distributed training on multiple GPUs or TPUs. Lines with # are then not needed\n",
    "accelerator = Accelerator() # New!\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(train_dataloader, eval_dataloader, model, optimizer) # New!\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in train_dataloader:\n",
    "      # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      # loss.backward()\n",
    "      accelerator.backward(loss) # New!\n",
    "      optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "      progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732e889-cb9e-44a8-acd3-e70633683f0a",
   "metadata": {},
   "source": [
    "In order to benefit from the speed-up offered by Cloud TPUs, samples should be padded to a fixed length with the ```padding=\"max_length\"``` and ```max_length``` arguments of the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46398e0-4707-41a7-9c1b-cbe32c68d9e2",
   "metadata": {},
   "source": [
    "- Previous code in a train.py make it runnable in any distributed setup.\n",
    "- ```accelerate config``` command creates a configuration file after asking some questions, so later ```accelerate launch train.py``` command can be launched.\n",
    "- This can be run in a Notebook using ```from accelerate import notebook_launcher``` and ```notebook_launcher(training_function)```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
