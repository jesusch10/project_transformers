{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15fba18-2e13-4f6d-a706-ee30ad59c054",
   "metadata": {},
   "source": [
    "Training a tokenizer is a statistical process that tries to identify which subwords are the best to pick for a given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35b493a5-9627-4154-9b17-eb846a224f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 10:44:30.068199: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748681070.157280     126 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748681070.180365     126 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748681070.365532     126 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748681070.365565     126 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748681070.365567     126 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748681070.365569     126 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-31 10:44:30.387931: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "from tokenizers import (decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer)\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ecf92-dc56-4590-abf8-ea097abee8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13285ad-3b18-4437-9978-31cb1f5a625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 1,000 texts at a time will be loaded\n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765555bb-e98d-47a8-abfc-cdc35807dd5e",
   "metadata": {},
   "source": [
    "Even though we are going to train a new tokenizer, it’s a good idea to do this to avoid starting entirely from scratch; the only thing that will change is the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d448830d-0aca-4c8d-abf5-90df6fd045c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self', ',', 'Ġinput', '_', 'size', ',', 'Ġoutput', '_', 'size', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ġoutput', '_', 'size', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'bias', 'Ġ=', 'Ġtorch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ĊĊĠĠĠ', 'Ġdef', 'Ġ__', 'call', '__(', 'self', ',', 'Ġx', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'bias', 'ĊĠĠĠĠ']\n"
     ]
    }
   ],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) # Only works with fast tokenizer (backed by Tokenizers instead of pure Python code)\n",
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")\n",
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens) # Just the tokens as a list\n",
    "encoding = tokenizer(example) \n",
    "print(type(encoding)) # The output of a tokenizer is a BatchEncoding object (subclass of a dict with additional methods for fast tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652442cb-30c2-4613-93c4-5599144f3304",
   "metadata": {},
   "source": [
    "Special symbols Ġ and Ċ that denote spaces and newlines, but we can also see that our tokenizer learned some tokens that are highly specific to a corpus of Python functions: for example, there is a ĊĠĠĠ token that represents an indentation, and a Ġ\"\"\" token that represents the three quotes that start a docstring. The tokenizer also correctly split the function name on _. This is quite a compact representation. In addition to the token corresponding to an indentation, here we can also see a token for a double indentation: ĊĠĠĠĠĠĠĠ. The special Python words like class, init, call, self, and return are each tokenized as one token, and we can see that as well as splitting on _ and . the tokenizer correctly splits even camel-cased names: LinearLayer is tokenized as [\"ĠLinear\", \"Layer\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8fb85-568f-46a8-82e2-bf4efa9aece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch encoding\n",
    "print(tokenizer.is_fast) #1 True or false\n",
    "print(tokens.is_fast) #2 True or false\n",
    "print(encoding.word_ids()) # Get the index of the word each token, where [CLS] and [SEP] are mapped to None\n",
    "start, end = encoding.word_to_chars(3) # Fetch tokenized work at index 3\n",
    "print(example[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966cbf70-1ac1-4bd5-a74d-c8f55f696e42",
   "metadata": {},
   "source": [
    "### Token-classification pipeline\n",
    "It handles entities that span over several tokens using a label for the beginning and another for the continuation of an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55440151-d4de-4ab0-b7bf-8cfc210133ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': np.float32(0.99938285),\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99815494),\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9959072),\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99923277),\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.97389334),\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.97611505),\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9887977),\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': np.float32(0.9932106),\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier = pipeline(\"token-classification\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1954fc68-9797-4bad-8c3a-9c296bab320f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9981694),\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9796019),\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9932106),\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\", device=-1) # device tells to use CPU (GPU not available for me)\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc6161-8618-47ce-9d5a-0a89482610f3",
   "metadata": {},
   "source": [
    "- `simple`: the score is just the mean of the scores of each token in the given entity.\n",
    "- `first`: the score of each entity is the score of the first token.\n",
    "- `max`: the score of each entity is the maximum score of the tokens in that entity.\n",
    "- `average` the score of each entity is the average of the scores of the words composing that entity, so for one world the score is the same as `simple`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eec51cf6-320d-47f8-a9da-10a7019d49eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},\n",
      " {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'}, \n",
      " {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'}, \n",
      " {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'}, \n",
      " {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'}, \n",
      " {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'}, \n",
      " {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'}, \n",
      " {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]\n"
     ]
    }
   ],
   "source": [
    "# Post-processing the predictions while grouping entities\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True) # When true creates a tuple in offset_mapping with start and end character positions of the token in the original text\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "outputs = model(**inputs_with_offsets) # outputs is a ModelOutput (dict-like) object\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist() # Shape of logits is (batch_size, sequence_length, num_labels)\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred] # Converting the numeric label to its string form using the model config\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx] # Getting the starting character index of the entity (from the first token).\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\" # Keep collecting tokens as long as they are labeled I-{label}\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e26b67-2499-480e-8c6a-33be91b40dbe",
   "metadata": {},
   "source": [
    "### Question-answering task\n",
    "It handles long contexts splitting the context into several parts (with overlap to avoid that answer would be split across two parts) and finds the maximum score for an answer in each part (average will not make sense as some parts of the context won't include the answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ee4a3-44fc-4f9d-9810-ea2684225466",
   "metadata": {},
   "source": [
    "- More info on question answering task: https://huggingface.co/learn/llm-course/chapter6/3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d9573-0ba4-488e-b6a1-99d2c145bd85",
   "metadata": {},
   "source": [
    "### Normalization and pre-tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450d20f-3f2d-4eb1-adef-3b25306975db",
   "metadata": {},
   "source": [
    "- The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents.\n",
    "- Pre-tokenization implies to split the texts into small entities, like words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "813fccfb-1d7c-4af0-83e8-fd0f5a82f295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n",
      "[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]\n"
     ]
    }
   ],
   "source": [
    "# See how a fast tokenizer performs this two tasks\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\n",
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850ad75-aac7-4678-add8-041d6be449f9",
   "metadata": {},
   "source": [
    "| Feature        | BPE                                                                 | WordPiece                                                                                                                        | Unigram                                                                                              |\n",
    "|----------------|---------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|\n",
    "| Training       | Starts from a small vocabulary and learns rules to merge tokens    | Starts from a small vocabulary and learns rules to merge tokens                                                                 | Starts from a large vocabulary and learns rules to remove tokens                                     |\n",
    "| Training step  | Merges the tokens corresponding to the most common pair            | Merges the tokens corresponding to the pair with the best score based on frequency, privileging pairs where individual tokens are less frequent | Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus    |\n",
    "| Learns         | Merge rules and a vocabulary                                        | Just a vocabulary                                                                                                                 | A vocabulary with a score for each token                                                             |\n",
    "| Encoding       | Splits a word into characters and applies the merges learned during training | Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word | Finds the most likely split into tokens, using the scores learned during training                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47edcfbc-32bf-4f68-acdc-2c9cba4ac4db",
   "metadata": {},
   "source": [
    "- Byte-Pair Encoding tokenization: https://huggingface.co/learn/llm-course/chapter6/5?fw=pt\n",
    "- WordPiece tokenization: https://huggingface.co/learn/llm-course/chapter6/6\n",
    "- Unigram tokenization: https://huggingface.co/learn/llm-course/chapter6/7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb309b6-b7b0-48f0-840e-46bb037b8855",
   "metadata": {},
   "source": [
    "###  Building a WordPiece tokenizer from scratch\n",
    "(Hello how are U tday?) -> Normalization -> (hello how are u tday?) -> Pre-tokenization into words -> ([hello, how, are, u, tday, ?]) -> Model -> ([hello, how, are, u, td, ##ay, ?]) -> Postprocessor -> ([CLS, hello, how, are, u, td, ##ay, ?, SEP])  \n",
    "  \n",
    "\\## is a subword prefix that indicates a token is a continuation of the previous word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e1f02b-0171-429f-bd78-7698e27e7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\")) # unk_token must be specified so the model knows what to return when it encounters characters it hasn’t seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a8b2ba-c730-4432-a596-555049af8094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "# Normalization from...\n",
    "\n",
    "# Existing model\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "\n",
    "# Scratch\n",
    "tokenizer.normalizer = normalizers.Sequence( # You can compose several normalizers using a Sequence\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()] # NFD Unicode normalizer allows StripAccents normalizer to recognize the accented characters\n",
    ")\n",
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc11710-330f-4936-b93d-0cef296e0eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Let', (0, 3)), (\"'\", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)), ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]\n"
     ]
    }
   ],
   "source": [
    "# Pre-tokenization from...\n",
    "\n",
    "# ... existing model\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "\n",
    "# ... scratch\n",
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()] # Splits on whitespace and all characters that are not letters, digits, or the underscore character. Use WhitespaceSplit() for only white spaces\n",
    ")\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d683ae-b2b0-4300-b276-b18f7f211fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
     ]
    }
   ],
   "source": [
    "# Model training from...\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "\n",
    "# ... iterator\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "\n",
    "# ... file\n",
    "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(dataset)):\n",
    "        f.write(dataset[i][\"text\"] + \"\\n\")\n",
    "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)\n",
    "\n",
    "# Testing\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)\n",
    "print(\"['let', \\\"'\\\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b722d-1e7d-40b8-b7ca-e6b2f205247c",
   "metadata": {},
   "source": [
    "Other parameters are in_frequency (the number of times a token must appear to be included in the vocabulary) and change the continuing_subword_prefix (if we want to use something different from ##)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9949f20-7121-4de7-93c7-d9831ea47dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n",
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Post-processing\n",
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "# BERT template\n",
    "tokenizer.post_processor = processors.TemplateProcessing( # We have to specify how to treat a single sentence and a pair of sentences\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")\n",
    "\n",
    "# Testing\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6002c55-39c2-4d91-9cee-7439614a4716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"let's test this tokenizer... on a pair of sentences.\"\n"
     ]
    }
   ],
   "source": [
    "# Last steps\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\") # Including a decoder\n",
    "tokenizer.decode(encoding.ids) # Testing\n",
    "tokenizer.save(\"tokenizer.json\") # Saving\n",
    "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\") # Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010c914-2f8e-4118-889d-a66232e12c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper the raw tokenizer into something compatible with transformers ...\n",
    "# .. using a specific tokenizer class\n",
    "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer) # Must be specified the special tokens that are different from the default ones (here, none):\n",
    "\n",
    "# ... using PreTrainedTokenizerFast\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\", # Key here is that class can’t infer from the tokenizer object which token is the mask token, the [CLS] token, etc.\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36577322-ed49-45d0-92c1-04af2bed3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Building a BPE tokenizer from scratch: Building a BPE tokenizer from scratch\n",
    "- Building a Unigram tokenizer from scratch: https://huggingface.co/learn/llm-course/chapter6/8#building-a-unigram-tokenizer-from-scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
