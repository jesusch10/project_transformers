{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a915e454-418f-4075-be18-5ab6f143d8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 11:42:56.398291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748511776.420060    1614 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748511776.425682    1614 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748511776.441830    1614 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748511776.441851    1614 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748511776.441853    1614 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748511776.441854    1614 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-29 11:42:56.447216: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e33128-6614-427e-9b15-06e715cb12cc",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2629b-a19d-4772-82ef-b84b36010f35",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ef924b-8aeb-46ca-9b73-bc70e52d4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching model’s tokenizer and caching it\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7f3d79-843d-48ff-8a6f-0203dcbc1adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c79504-614b-4334-95ce-db883beacc63",
   "metadata": {},
   "source": [
    "The output is a dictionary containing:\n",
    "- input_ids: unique identifiers of the tokens in each sentence\n",
    "- attention_mask:\n",
    "  \n",
    "It can be saved with ```tokenizer.save_pretrained(\"directory_on_my_computer\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdd641bc-2241-41e0-9a1e-dc5c293bac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.', 'i', 'hate', 'this', 'so', 'much', '!']\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 1045, 5223, 2023, 2061, 2172, 999]\n",
      "i've been waiting for a huggingface course my whole life. i hate this so much!\n"
     ]
    }
   ],
   "source": [
    "# How words are tokenized and how ids can be matched back to vocabulary\n",
    "tokens = tokenizer.tokenize(sequences)\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d1f92c-8b69-4304-92bd-df764df239ec",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8251278-47ba-4d55-814e-663a50d73cf5",
   "metadata": {},
   "source": [
    "### Automodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02bc3a28-668a-4094-951b-58bd30caa950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e90428cbe7642699bae4445a4b7a92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same way to download the pretrained model\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b782eb-13cd-40b0-8e4c-0f1827dff252",
   "metadata": {},
   "source": [
    "The AutoModel class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It’s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e86b4512-a744-4edd-9c13-cc519e21956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36459bc-0e00-4ef9-840e-2cff51a8cfd4",
   "metadata": {},
   "source": [
    "- Batch size: The number of sequences processed at a time (2 in our example).\n",
    "- Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "- Hidden size: The vector dimension of each model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "709b3997-e480-461e-8ff6-6dc962be0035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Model with a sequence classification head\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5f299-adf5-4dd4-91a7-80609afc4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing number of parameters in the model\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel() # Sums the count of all elements in a certain parameter tensor\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c47f536a-2d57-4a0e-b25f-d4f893f85a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Logits, unnormalized scores, are converted to probabilities through a SoftMax layer\n",
    "print(outputs.logits)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05c24722-2fc8-4b7c-ba78-effc44ab1c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the labels corresponding to each position,\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37faa7-6838-446c-be0f-aff8e4f0ce36",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2759226-d4e4-4f21-88d1-174774a4ae90",
   "metadata": {},
   "source": [
    "### Non-automodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343e7ddd-d68f-4c27-9520-c5d531a01b68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading configuration to the model\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff99454-a079-4b51-81a1-7c1050d5b725",
   "metadata": {},
   "source": [
    "The model needs to be trained first (a lot of time), but a loaded version can be fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c48565-81cd-4de1-9af3-b5d07ca788fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1abecf85ca4670ac0beeffb8b219a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34751099ef2f4465b22fda24d664eeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading a Transformer model that is already trained\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7219f880-c15a-4228-a3a3-00ad700ba74a",
   "metadata": {},
   "source": [
    "More Bert checkpoints in https://huggingface.co/models?other=bert. Unlike automodels (architecture-agnostic), only Bert checkpoinys will work with Bert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb463f-ecd5-41b9-ad3b-3c0746ebe787",
   "metadata": {},
   "source": [
    "Saving a model with ```model.save_pretrained(\"directory_on_my_computer\")``` creates two files:\n",
    "- config.json: architecture attributes and metadata\n",
    "- pytorch_model.bin: state dictionary with weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df287ba5-fb35-4b92-ad8d-113e9a2799ce",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d71ee-0531-46a5-89df-bb6e0782d417",
   "metadata": {},
   "source": [
    "### Handling multiple sequences - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6f4efa5-8b32-4253-ac08-d985c5fda73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids]) # Transformers models expect multiple sentences by default\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2677a34-e8d8-471d-8bee-43c3c5baceaa",
   "metadata": {},
   "source": [
    "Batching is the act of sending multiple sentences through the model, all at once. Batches must have a rectangular shape, so padding us used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72fe60dc-aa0d-4914-810a-d40515d7f3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8504101-bce1-4ddc-8168-d36cdd47fe28",
   "metadata": {},
   "source": [
    "There’s something wrong with the logits in our batched predictions in the second row: different values were obtained because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3e3e198-ed63-47c4-81ea-1044f4de08b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844eae75-8189-4b50-b06f-6d5250e1656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different paddings\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\") # Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\") # Will pad the sequences up to the model max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8) # Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, truncation=True) # Will truncate the sequences that are longer than the model max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True) # Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\") # Returns PyTorch (pt), TensorFlow (tf) or NumPy (np) tensors\n",
    "\n",
    "model_inputs = tokenizer(sequences, padding=True, truncation=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc66ebc-e960-4483-90e4-5950d4a216b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 1045, 5223, 2023, 2061, 2172, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.', '[SEP]', 'i', 'hate', 'this', 'so', 'much', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model_inputs = tokenizer(\"I've been waiting for a HuggingFace course my whole life.\",  # Sequences are handled as a pair so the model can predict if two sentences are paraphrases or not\n",
    "                         \"I hate this so much!\",\n",
    "                         padding=True, # Usually the two parameters are used:\n",
    "                         truncation=True)\n",
    "print(model_inputs)\n",
    "print(tokenizer.convert_ids_to_tokens(model_inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10383211-a254-4ab5-8d7a-ef4e71d79e44",
   "metadata": {},
   "source": [
    "The model added special words for pretraining to separate the phrases. Notice that ```token_type_ids``` has different IDs for each phrase. This key may not be on a different checkpoint. BERT in this case needs it for ```next sentence prediction```. However, ```token_type_ids``` is not compulsory as long as you use the same checkpoint for the tokenizer and the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cf1d53-0aea-4d7e-a22d-54fabc17ac60",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
