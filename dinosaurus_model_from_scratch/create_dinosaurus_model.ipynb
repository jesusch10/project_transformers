{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f86fd72-456a-4189-832a-5d98dc5ab255",
   "metadata": {},
   "source": [
    "# Creating a character level language model to generate new dinosaurus names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49153514-8fbe-49c0-b536-8e52432dbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "import numpy as np\n",
    "from rnn_lstm_blocks import *\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f132215-0a53-483c-83dd-488948a6e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data = open('data/dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb3a24d-022b-4d0b-ac25-cdc26203c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "\n",
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "    b = np.zeros((n_a, 1)) # hidden bias\n",
    "    by = np.zeros((n_y, 1)) # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "    \"\"\"\n",
    "    Updates the RNN parameters using gradient descent.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- Dictionary containing the RNN parameters:\n",
    "                  Wax -- Weight matrix for input to hidden state, shape (n_a, n_x)\n",
    "                  Waa -- Weight matrix for hidden to hidden state, shape (n_a, n_a)\n",
    "                  Wya -- Weight matrix for hidden to output, shape (n_y, n_a)\n",
    "                  b   -- Bias vector for hidden state, shape (n_a, 1)\n",
    "                  by  -- Bias vector for output, shape (n_y, 1)\n",
    "    \n",
    "    gradients -- Dictionary containing the gradients of the loss with respect to the parameters:\n",
    "                 dWax -- Gradient of Wax, shape (n_a, n_x)\n",
    "                 dWaa -- Gradient of Waa, shape (n_a, n_a)\n",
    "                 dWya -- Gradient of Wya, shape (n_y, n_a)\n",
    "                 db   -- Gradient of b, shape (n_a, 1)\n",
    "                 dby  -- Gradient of by, shape (n_y, 1)\n",
    "\n",
    "    lr -- Learning rate (a positive float), determines the step size in gradient descent\n",
    "\n",
    "    Returns:\n",
    "    parameters -- Updated dictionary of parameters after applying gradient descent\n",
    "    \"\"\"\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    loss, cache = rnn_forward_character_generation(X, Y, a_prev, parameters)\n",
    "    gradients, a = rnn_backward_character_generation(X, Y, parameters, cache)\n",
    "    gradients = clip(gradients, 5)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]\n",
    "\n",
    "\n",
    "def get_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0639fabf-f5cf-4b69-865c-4991b4c019e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  0 idx =  0\n",
      "single_example = turiasaurus\n",
      "single_example_chars ['t', 'u', 'r', 'i', 'a', 's', 'a', 'u', 'r', 'u', 's']\n",
      "single_example_ix [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]\n",
      " X =  [None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19] \n",
      " Y =        [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0] \n",
      "\n",
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Co\n",
      "Eyhqgyshubjerbjgmis\n",
      "Qfguyhvbqtdkxb\n",
      "Okuxhjmozrjgycsxqkeisqywvxgijorg\n",
      "Jhywwqinubuavuapmdctozzzcyflteoebci\n",
      "Kqjbagdzduoomziuojypckurxgtloyzwiwqk\n",
      "Tmutuhunlr\n",
      "j =  1535 idx =  1535\n",
      "j =  1536 idx =  0\n",
      "Iteration: 2000, Loss: 27.884160\n",
      "\n",
      "Amycgangwsacrusycripsiurgtorandprusayrabhustactgny\n",
      "Honqmeleprusamglonggpodanrosennnsteroshuceranelocr\n",
      "Inlonaleonxyorytonganytabtonosaunus\n",
      "Padeleloypurapfnorusaurus\n",
      "Aurus\n",
      "Itreriptixtisgurros\n",
      "Ontepokosauluspmurinontyaicallpeosaurus\n",
      "Iteration: 4000, Loss: 25.901815\n",
      "\n",
      "Amkorosaurus\n",
      "Stdoosaurus\n",
      "Rusaurus\n",
      "Eschuszuratanstipanos\n",
      "Chmmoop\n",
      "Alorosaurus\n",
      "Azarosaurus\n",
      "Iteration: 6000, Loss: 24.608779\n",
      "\n",
      "Crlhux\n",
      "Raptorfirus\n",
      "Deliosaurus\n",
      "Hua\n",
      "Fhidosaurushs\n",
      "S\n",
      "Eheosaurus\n",
      "Iteration: 8000, Loss: 24.070350\n",
      "\n",
      "Loninitan\n",
      "Sartosaurus\n",
      "Alicisaurus\n",
      "Rapistos\n",
      "Hingiung\n",
      "Tamethosmilus\n",
      "Cerapochantoensaurus\n",
      "Iteration: 10000, Loss: 23.844446\n",
      "\n",
      "Ceriondestes\n",
      "Taraxosaurus\n",
      "Ceniaschyatandomos\n",
      "Sitorong\n",
      "Xealochus\n",
      "Wuronarys\n",
      "Brovoradis\n",
      "Iteration: 12000, Loss: 23.291971\n",
      "\n",
      "Tonoshapuggosaurus\n",
      "Gkuenotes\n",
      "Meginsthuscenoder\n",
      "Teecredrnosaurus\n",
      "S\n",
      "Stophosposeangiceraton\n",
      "Daveesuraensauon\n",
      "Iteration: 14000, Loss: 23.382338\n",
      "\n",
      "Antarog\n",
      "Antoropertyla\n",
      "Yraptoraluele\n",
      "Ncosaurus\n",
      "Henothuangorosaurus\n",
      "Eradon\n",
      "Babborosaurus\n",
      "Iteration: 16000, Loss: 23.265759\n",
      "\n",
      "Edrhiu\n",
      "Bqrostan\n",
      "Indus\n",
      "Savater\n",
      "Erlalinis\n",
      "Iapopoph\n",
      "Glkosaurus\n",
      "Iteration: 18000, Loss: 22.901372\n",
      "\n",
      "Yloosaurus\n",
      "Hesaurus\n",
      "Sungala\n",
      "Sinosauroshoryphasaurus\n",
      "Enshanosaurus\n",
      "Mracosaurus\n",
      "Stlosaurus\n",
      "Iteration: 20000, Loss: 22.924847\n",
      "\n",
      "Provos\n",
      "Onwosaurus\n",
      "Ynimuakeratops\n",
      "Wlocrosaurus\n",
      "Laxyechyusaurus\n",
      "Hanypa\n",
      "Herykowosaurus\n",
      "Iteration: 22000, Loss: 22.759659\n",
      "\n",
      "Konomapsurenotakongeus\n",
      "Skepotatoceravops\n",
      "Psilenteteros\n",
      "Ykong\n",
      "Rayseleosaurus\n",
      "Praloceratops\n",
      "Adijetets\n"
     ]
    }
   ],
   "source": [
    "# Training and generating \n",
    "def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data_x -- text corpus, divided in words\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text (size of the vocabulary)\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    loss = -np.log(1.0/vocab_size)*dino_names\n",
    "    examples = [x.strip() for x in data_x]\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        idx = j % len(examples) # since num_iterations > examplles, idx wraps around to the beginning when it exceeds the number of examples\n",
    "        single_example = examples[idx]\n",
    "        single_example_chars = [c for c in single_example]\n",
    "        single_example_ix = [char_to_ix[c] for c in single_example_chars]\n",
    "        X = [None] + single_example_ix\n",
    "        ix_newline = char_to_ix['\\n']\n",
    "        Y = X[1:] + [char_to_ix['\\n']]\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "        loss = loss * 0.999 + curr_loss * 0.001 # to keep the loss smooth.\n",
    "        \n",
    "        # debug statements to aid in correctly forming X, Y\n",
    "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
    "            print(\"j = \" , j, \"idx = \", idx,) \n",
    "        if verbose and j in [0]:\n",
    "            print(\"single_example =\", single_example)\n",
    "            print(\"single_example_chars\", single_example_chars)\n",
    "            print(\"single_example_ix\", single_example_ix)\n",
    "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            for name in range(dino_names):\n",
    "                sampled_indices = sample(parameters, char_to_ix)\n",
    "                last_dino_name = get_sample(sampled_indices, ix_to_char)\n",
    "                print(last_dino_name.replace('\\n', ''))\n",
    "        \n",
    "    return parameters, last_dino_name\n",
    "\n",
    "parameters, last_name = model(data.split(\"\\n\"), ix_to_char, char_to_ix, 22001, verbose = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
