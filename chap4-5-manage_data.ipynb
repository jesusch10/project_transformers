{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b91718a1-f2f7-497f-a671-f1d37dc516ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import html\n",
    "import psutil\n",
    "import timeit\n",
    "from transformers import AutoTokenizer\n",
    "from itertools import islice\n",
    "from datasets import interleave_datasets\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b129582-7556-4054-ab3a-8c05aee87f8c",
   "metadata": {},
   "source": [
    "- Hugging Face API to share the model (the Hub is a simple Git repository): https://huggingface.co/learn/llm-course/chapter4/3\n",
    "- Proper model documentation to include: https://huggingface.co/learn/llm-course/chapter4/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595652c1-e755-4e06-b500-f57ff6c9744c",
   "metadata": {},
   "source": [
    "### Managing data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711bfbda-a5eb-4b1a-81c7-9c44c66b05db",
   "metadata": {},
   "source": [
    "Loading:  \n",
    "```data_type = ['csv', 'text', 'json', 'pandas']```  \n",
    "```dataset = load_dataset(data_type[0], data_files='my_file.xxx')```  \n",
    "For TSV, use ```csv``` and add the parameter ```delimiter='\\t'```  \n",
    "  \n",
    "Saving:  \n",
    "- Arrow format: ```dataset.save_to_disk()```  \n",
    "- CSV format: ```dataset.to_csv()```  \n",
    "- JSON format: ```dataset.to_json()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6db25a-1fe2-4a0c-9298-74874c27ddd5",
   "metadata": {},
   "source": [
    "Now three modifications:\n",
    "- By default, loading local files creates a DatasetDict object with a train split, but to also include the test split.\n",
    "- Datasets supports automatic decompression of the input files.\n",
    "- Loading remote files is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a6be8-e836-4b23-a8c5-bfb08a16d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {'train': url + 'SQuAD_it-train.json.gz', 'test': url + 'SQuAD_it-test.json.gz'}\n",
    "dataset = load_dataset('json', data_files=data_files, field='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9da49-32ea-44ab-99a3-3700cf425701",
   "metadata": {},
   "source": [
    "### Inspecting and correcting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a1292f-2bfd-4cb7-8951-f91a30b54605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data and inspecting type of data (good practice)\n",
    "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!unzip drugsCom_raw.zip\n",
    "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141bbf47-95ed-4a39-9752-139b8f7fdbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting 'Unnamed: 0' column which looks suspiciously like an anonymized ID for each patient\n",
    "for split in drug_dataset.keys():\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))\n",
    "drug_dataset = drug_dataset.rename_column(original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fb48b1b-956c-48b5-b601-58d170c5c319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b006084ec6a7416f9b7ce483853fd276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/161297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715ec408efba4ef4b39eec04e566712c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/53766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5984570ab9964d52b17bab7233eecfc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e259c57bff1a4bb7bec9afbdab863f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 160398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing condition labels\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "def lowercase_condition(example):\n",
    "    return {\"condition\": example[\"condition\"].lower()}\n",
    "\n",
    "drug_dataset.map(lowercase_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b41443ee-0189-4591-8f0e-abd7ab461609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': [111469, 13653, 53602],\n",
       " 'drugName': ['Ledipasvir / sofosbuvir',\n",
       "  'Amphetamine / dextroamphetamine',\n",
       "  'Alesse'],\n",
       " 'condition': ['Hepatitis C', 'ADHD', 'Birth Control'],\n",
       " 'review': ['\"Headache\"', '\"Great\"', '\"Awesome\"'],\n",
       " 'rating': [10.0, 10.0, 10.0],\n",
       " 'date': ['February 3, 2015', 'October 20, 2009', 'November 23, 2015'],\n",
       " 'usefulCount': [41, 3, 0],\n",
       " 'review_length': [1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the number of words in each review.\n",
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())} # Returns a dictionary whose key does not correspond to one of the column names in the dataset, so Dataset.map() creates a new review_length column\n",
    "\n",
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "drug_dataset[\"train\"].sort(\"review_length\")[:3] # Inspecting extreme values (reviews with one word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3706ac75-91b5-4cc6-bc1e-5367991e468a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec35a62f4a784e9997d72f1b44dfd59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89806ff7cbd44d6c9740678d6fafdfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 138514, 'test': 46108}\n"
     ]
    }
   ],
   "source": [
    "# Eliminating reviews with less than 30 words (not optimal for sentimental analysis)\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows) # This has removed around 15% of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c97f5fe6-e60e-40ea-ba61-613f1a23a0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0babeb30474e4b873173b060080616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c1a8cc3fa146e49e6cb01d4904c9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 578 ms, sys: 331 ms, total: 909 ms\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "# Unescaping HTML characters\n",
    "text = \"I&#039;m a transformer called BERT\"\n",
    "html.unescape(text)\n",
    "%time new_drug_dataset = drug_dataset.map(lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True, num_proc=8) # Batched allows several elements to be processed at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d4839-0119-438b-acf3-1fa80032d227",
   "metadata": {},
   "source": [
    "### Conversion between various third-party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "293b8182-b0c9-4d71-8401-65a7074b499a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id    drugName      condition  \\\n",
       "0       95260  Guanfacine           ADHD   \n",
       "1       92703      Lybrel  Birth Control   \n",
       "2      138000  Ortho Evra  Birth Control   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"My son is halfway through his fourth week of ...     8.0   \n",
       "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "2  \"This is my first time using any form of birth...     8.0   \n",
       "\n",
       "                date  usefulCount  review_length  \n",
       "0     April 27, 2010          192            141  \n",
       "1  December 14, 2009           17            134  \n",
       "2   November 3, 2015           10             89  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Pandas\n",
    "drug_dataset.set_format(\"pandas\")\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "595fd429-2bef-4f03-9fa2-20a6a39ac93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['condition', 'frequency'],\n",
       "    num_rows: 819\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From Pandas\n",
    "train_df = drug_dataset[\"train\"][:] # Ee need to slice the whole dataset to obtain a pandas.DataFrame\n",
    "frequencies = (\n",
    "    train_df[\"condition\"]\n",
    "    .value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"condition\", \"count\": \"frequency\"})\n",
    ")\n",
    "freq_dataset = Dataset.from_pandas(frequencies)\n",
    "freq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df00e810-ebec-4c32-ad3b-81ec4b5fea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseting the output format \n",
    "drug_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc9848-ff14-4e89-9ba1-f763c752b395",
   "metadata": {},
   "source": [
    "### Creating a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "451a75ca-bf62-4ebd-b2b3-4db79b911c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b51d7-a6fe-4249-bbf1-6056e1ccc8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each split is stored as a separate file\n",
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"drug-reviews-{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe4a85-d836-4eac-88c8-f5132f9206eb",
   "metadata": {},
   "source": [
    "### Managing big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3de811-8303-4d0c-be81-e81ff28916b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few minutes to run\n",
    "data_files = \"https://huggingface.co/datasets/casinca/PUBMED_title_abstracts_2019_baseline/resolve/main/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
    "pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
    "size_gb = pubmed_dataset.dataset_size / (1024**3)\n",
    "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\") # Bytes to megabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c6f2e-ca46-4073-943b-077e29cb3601",
   "metadata": {},
   "source": [
    "Thanks to pyarrow library (Apache Arrow memory format), Datasets treats each dataset as a memory-mapped file, which provides a mapping between RAM and filesystem storage that allows the library to access and operate on elements of the dataset across multiple processes without needing to fully load it into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ae1c2e0-6157-44fc-bc94-a8b8e584d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /home/jesusch10/miniconda3/envs/project_transformers_env/lib/python3.10/site-packages (7.0.0)\n"
     ]
    }
   ],
   "source": [
    "code_snippet = \"\"\"batch_size = 1000\n",
    "\n",
    "for idx in range(0, len(pubmed_dataset), batch_size):\n",
    "    _ = pubmed_dataset[idx:idx + batch_size]\n",
    "\"\"\"\n",
    "\n",
    "time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n",
    "print(\n",
    "    f\"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in \"\n",
    "    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b646026-5b16-4ede-b684-af79349e7733",
   "metadata": {},
   "source": [
    "```streaming=True``` argument in the load_dataset() function enables dataset streaming. The object returned is an IterableDataset, which allows to access the elements of an IterableDataset if iterating over it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfe9c0-0017-42b7-8a4a-5b9e1356c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_dataset_streamed = load_dataset(\"json\", data_files=data_files, split=\"train\", streaming=True) # Not loading the whole dataset \n",
    "next(iter(law_dataset_streamed)) # Peeking at a single item without loading everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057437e-ab7f-4159-adf6-262da731c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to iterate over the suffle\n",
    "shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42) # Shuffling the elements in a predefined buffer_size\n",
    "next(iter(shuffled_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ee20b-fe6d-4694-a729-06240a4b81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar way to Dataset.select() for IterableDataset\n",
    "train_dataset = shuffled_dataset.skip(1000) # Skip the first 1,000 examples and include the rest in the training set\n",
    "validation_dataset = shuffled_dataset.take(1000) # Take the first 1,000 examples for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9aa69f-2b0b-447d-8b92-542565a00dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining two big dataset\n",
    "combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed]) # law_dataset_streamed not available any more in https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst)\n",
    "list(islice(combined_dataset, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9e7bf-3fd9-4c9d-890e-7af1ff2a724c",
   "metadata": {},
   "source": [
    "### Creating a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359b3e9-0eef-4040-b2c7-4abbeca0cdcc",
   "metadata": {},
   "source": [
    "The comments associated with an issue in GitHub provide a rich source of information, especially if we’re interested in building a search engine to answer user queries about the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b911e26-3416-4c7d-b7c9-f85eaa771bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/7590',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7590/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7590/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7590/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/issues/7590',\n",
       "  'id': 3101654892,\n",
       "  'node_id': 'I_kwDODunzps64339s',\n",
       "  'number': 7590,\n",
       "  'title': '`ArrowNotImplementedError: Unsupported cast from list<item:struct<…>> to struct` when loading nested `Sequence(Features)` JSONL',\n",
       "  'user': {'login': 'AHS-uni',\n",
       "   'id': 183279820,\n",
       "   'node_id': 'U_kgDOCuygzA',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/183279820?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/AHS-uni',\n",
       "   'html_url': 'https://github.com/AHS-uni',\n",
       "   'followers_url': 'https://api.github.com/users/AHS-uni/followers',\n",
       "   'following_url': 'https://api.github.com/users/AHS-uni/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/AHS-uni/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/AHS-uni/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/AHS-uni/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/AHS-uni/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/AHS-uni/repos',\n",
       "   'events_url': 'https://api.github.com/users/AHS-uni/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/AHS-uni/received_events',\n",
       "   'type': 'User',\n",
       "   'user_view_type': 'public',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 1,\n",
       "  'created_at': '2025-05-29T22:53:36Z',\n",
       "  'updated_at': '2025-05-30T09:02:12Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'NONE',\n",
       "  'type': None,\n",
       "  'active_lock_reason': None,\n",
       "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
       "  'body': '### Describe the bug\\n\\nWhen loading a JSONL dataset with a top‐level `tags` field defined as a list of structs (via `Sequence(Features({...}))`), Hugging Face Datasets still infers `tags` as a single `struct<…>`, and then PyArrow throws:\\n\\n```\\nArrowNotImplementedError: Unsupported cast from list<item: struct<…>> to struct using function cast_struct\\n```\\n\\neven when the full nested `Features` schema is passed to `load_dataset(..., features=…)` and `streaming=False`. This prevents loading any entries where `tags` has more than one element (or is empty).\\n\\n### Steps to reproduce the bug\\n\\n[Colab Link](https://colab.research.google.com/drive/1FZPQy6TP3jVd4B3mYKyfQaWNuOAvljUq#scrollTo=qkigsmEZLrnY)\\n\\n### Expected behavior\\n\\nThe `tags` field should be recognized as `list<struct<name,target,comment>>`, and both empty lists (`[]`) and multi-element lists should load without casting errors.\\n\\n### Environment info\\n\\n* **datasets** version: `3.6.0`\\n* **pyarrow** version: `20.0.0`\\n* **Python**: 3.12.10\\n* **OS**: Ubuntu 24.04.02 LTS ',\n",
       "  'closed_by': None,\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7590/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7590/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
    "response = requests.get(url)\n",
    "print(response.status_code)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fa7ca-060f-4114-88fe-c8cb686dac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the GitHub REST API to fetch up issues, avoiding hitting API rate limits, and storing the issues as JSON\n",
    "with open(\"api_token.txt\", \"r\") as f:\n",
    "    token = f.readline().strip()\n",
    "headers = {\"Authorization\": f\"token {token}\"}\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=10_000,\n",
    "    rate_limit=5_000,\n",
    "    issues_path=Path(\".\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100  # Number of issues to return per page (max API value)\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json()) # Add the list of issues from this page to batch\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []  # Flush batch for next time period\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b206be1a-02a1-47d4-beb4-ef32f96ab350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Running defined API (long time)\n",
    "fetch_issues()\n",
    "issues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01be8c62-fecb-4fc4-9ae6-137febc273c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a09ed3559164db584d6f80ee71d4b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46da37dcc861421b8fe4ecf87634951b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "datasets-issues-with-comments.jsonl:   0%|          | 0.00/12.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf13b9a89adb4d6d9412ed183f719850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 Alternative (faster)\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380654c-8274-437d-aca7-c66033425155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns all the comments associated with an issue (already included in lewtun/github-issues)\n",
    "def get_comments(issue_number):\n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return [r[\"body\"] for r in response.json()]\n",
    "\n",
    "issues_with_comments_dataset = issues_dataset.map(\n",
    "    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c1f0e-cb7b-41e7-8700-5f9a22b375d2",
   "metadata": {},
   "source": [
    "There are several thousand issues because the API also considers every pull request an issue; the ```pull_request``` is associated with various URLs for pull requests, while ordinary issues have a None entry, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ddecf210-9a9d-4daa-82bb-4cfcb370cc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f97d6faafb4823be3139257c5d2990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0))\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960eaadb-7213-42f6-9cb1-669a31f5998c",
   "metadata": {},
   "source": [
    "Although we could proceed to further clean up the dataset by dropping or renaming some columns, it is generally a good practice to keep the dataset as “raw” as possible at this stage so that it can be easily used in multiple applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a01c16-60a2-4091-a23f-fc797afaf9e4",
   "metadata": {},
   "source": [
    "### Embeddings to develop a semantic search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243ae20-5dc7-4fa1-aef2-1272dbdc59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most informative columns for embeddings are title, body, and comments, while html_url provides us with a link back to the source issue\n",
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bef05c-82a4-491e-b336-82017e8c1865",
   "metadata": {},
   "source": [
    "Because our comments column is currently a list of comments for each issue, we need to “explode” the column so that each row consists of an (html_url, title, body, comment) tuple; in Pandas we can do this with the DataFrame.explode() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99e39b74-be60-4231-a0ab-d1ff2769b21b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repository_url</th>\n",
       "      <th>labels_url</th>\n",
       "      <th>comments_url</th>\n",
       "      <th>events_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>author_association</th>\n",
       "      <th>active_lock_reason</th>\n",
       "      <th>pull_request</th>\n",
       "      <th>body</th>\n",
       "      <th>timeline_url</th>\n",
       "      <th>performed_via_github_app</th>\n",
       "      <th>is_pull_request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>1000624883</td>\n",
       "      <td>I_kwDODunzps47pFLz</td>\n",
       "      <td>2945</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>...</td>\n",
       "      <td>1632120421000</td>\n",
       "      <td>1632139287000</td>\n",
       "      <td>1.632139e+12</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>1000624883</td>\n",
       "      <td>I_kwDODunzps47pFLz</td>\n",
       "      <td>2945</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>...</td>\n",
       "      <td>1632120421000</td>\n",
       "      <td>1632139287000</td>\n",
       "      <td>1.632139e+12</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>1000355115</td>\n",
       "      <td>I_kwDODunzps47oDUr</td>\n",
       "      <td>2943</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>...</td>\n",
       "      <td>1632068197000</td>\n",
       "      <td>1632155143000</td>\n",
       "      <td>1.632155e+12</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>1000355115</td>\n",
       "      <td>I_kwDODunzps47oDUr</td>\n",
       "      <td>2943</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>...</td>\n",
       "      <td>1632068197000</td>\n",
       "      <td>1632155143000</td>\n",
       "      <td>1.632155e+12</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                      repository_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datasets   \n",
       "1  https://api.github.com/repos/huggingface/datasets   \n",
       "2  https://api.github.com/repos/huggingface/datasets   \n",
       "3  https://api.github.com/repos/huggingface/datasets   \n",
       "\n",
       "                                          labels_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                        comments_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                          events_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                            html_url          id  \\\n",
       "0  https://github.com/huggingface/datasets/issues...  1000624883   \n",
       "1  https://github.com/huggingface/datasets/issues...  1000624883   \n",
       "2  https://github.com/huggingface/datasets/issues...  1000355115   \n",
       "3  https://github.com/huggingface/datasets/issues...  1000355115   \n",
       "\n",
       "              node_id  number  \\\n",
       "0  I_kwDODunzps47pFLz    2945   \n",
       "1  I_kwDODunzps47pFLz    2945   \n",
       "2  I_kwDODunzps47oDUr    2943   \n",
       "3  I_kwDODunzps47oDUr    2943   \n",
       "\n",
       "                                               title  ...     created_at  \\\n",
       "0                              Protect master branch  ...  1632120421000   \n",
       "1                              Protect master branch  ...  1632120421000   \n",
       "2  Backwards compatibility broken for cached data...  ...  1632068197000   \n",
       "3  Backwards compatibility broken for cached data...  ...  1632068197000   \n",
       "\n",
       "      updated_at     closed_at  author_association active_lock_reason  \\\n",
       "0  1632139287000  1.632139e+12              MEMBER               None   \n",
       "1  1632139287000  1.632139e+12              MEMBER               None   \n",
       "2  1632155143000  1.632155e+12         CONTRIBUTOR               None   \n",
       "3  1632155143000  1.632155e+12         CONTRIBUTOR               None   \n",
       "\n",
       "  pull_request                                               body  \\\n",
       "0         None  After accidental merge commit (91c55355b634d0d...   \n",
       "1         None  After accidental merge commit (91c55355b634d0d...   \n",
       "2         None  ## Describe the bug\\r\\nAfter upgrading to data...   \n",
       "3         None  ## Describe the bug\\r\\nAfter upgrading to data...   \n",
       "\n",
       "                                        timeline_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "   performed_via_github_app  is_pull_request  \n",
       "0                      None            False  \n",
       "1                      None            False  \n",
       "2                      None            False  \n",
       "3                      None            False  \n",
       "\n",
       "[4 rows x 28 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]\n",
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0587f418-046f-473b-b6ce-91eb9c702bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0036dbfceba43beb0a7ef0e37712368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969921c57ded46a4b70a79838880c042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filtering short comments\n",
    "comments_dataset = comments_dataset.map(lambda x: {\"comment_length\": len(x[\"comments\"].split())})\n",
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2639cdd2-ce98-4a40-a547-1540604c59f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0520f3d343448ea5ec30d679b009a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatenating the issue title, description, and comments together in a new text column\n",
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "010f3701-abf4-40e7-b009-c4c06b13273a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b86a0292f1c43ef949477b901fe86d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522614f74d9d4bf59153b0d573820bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df8c7c4e5044ef8ba7e9df0f8840d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292c87915cd542f38902d4443fde7609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83db2958f6545c698dcd09efd8a512a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 23:10:06.856783: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748639407.055915     129 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748639407.077333     129 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748639407.276539     129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748639407.276592     129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748639407.276594     129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748639407.276596     129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-30 23:10:07.294941: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88390a79e2943ed93b4226ff9401511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\") # \"cuda\" is not available in my PC\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\" # Dedicated to creating embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee4ad6-f8bb-4914-8ce3-b5ab64506f83",
   "metadata": {},
   "source": [
    "Transformers output an embedding for each token, but we want one vector per whole issue, so we need to combine (or pool) all those token embeddings into one vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c786bd2-02c2-458a-9305-366f307ecb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\"    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n",
    "\n",
    "embeddings_dataset = comments_dataset.map(lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aad6d6-6028-4e15-a8f0-44b2c619acee",
   "metadata": {},
   "source": [
    "- ```.detach()``` detaches the tensor from the computation graph so it no longer tracks gradients.\n",
    "- ```.cpu()``` moves the tensor to the CPU, in case it's still on the GPU.\n",
    "- ```.numpy()``` converts the PyTorch tensor to a NumPy array, required by Datasets when trying to index them with FAISS (next section).\n",
    "- ```[0]``` takes the first (and only) row of the array, since this was a batch of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb400591-44dd-42cc-b4f0-449aa16a54c4",
   "metadata": {},
   "source": [
    "The basic idea behind FAISS (short for Facebook AI Similarity Search) is to create a special data structure called an index that allows one to find which embeddings are similar to an input embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5e4c2-9323-4f6b-b749-abf0c0d93685",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347769f-a91a-48b3-b3df-f5c25f0205a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a query\n",
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481758d-e9ce-416f-b920-bccc3e6d441a",
   "metadata": {},
   "source": [
    "Just like with the documents, we now have a 768-dimensional vector representing the query, which we can compare against the whole corpus to find the most similar embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500748fa-c279-4217-bd54-ac8e680b6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a query\n",
    "scores, samples = embeddings_dataset.get_nearest_examples(\"embeddings\", question_embedding, k=5)\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
