# Advanced Large Language Models (LLMs):

This repository documents my hands-on learning journey in building and understanding Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures. It includes both foundational implementations and practical applications using state-of-the-art tools and libraries.

The work is inspired and guided by the following key learning resources:

- [**LLM Course** by Hugging Face](https://huggingface.co/learn/llm-course/)
- [**Generative AI with Large Language Models Course** by AWS](https://www.coursera.org/learn/generative-ai-with-llms)
- [**Sequence Models Course** by DeepLearning.AI](https://www.coursera.org/learn/nlp-sequence-models), part of the **Deep Learning Specialization**

## 📁 Structure

```bash
📂 LLMs_models_from_hugging_ecosystem/
    └── I summarized the Transformers and Hugging Face workflows to train, fine-tune, and manage LLMs
📂 dinosaurus_model_from_scratch/
    └── I created a RNN model from scratch to generate new dinosaurus names
📂 jazz_model_from_keras/
    └── I created a LSTM model using Keras to generate Jazz music
📂 transformer_model_from_tensorflow/
    └── I built the Transformer architecture using TensorFlow ready for training

